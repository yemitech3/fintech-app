Steps for Delivering a DevOps Project:
fintech-app: https://github.com/ndiforfusi/fintech-app

1. Project Planning and Setup:

Define responsibilities and task allocation

Split your team of engineers as needed (e.g., Infrastructure, CI/CD, Application Deployment).

Optional: Clearly document responsibilities in a shared workspace (Confluence, or GitHub Wiki).


Provision access:

Create credentials for each engineer in the target AWS account.

Assign least-privilege IAM roles (use predefined groups like DevOpsAdmin, Developer, etc.).

Store credentials securely (AWS Secrets Manager).


Optional GitHub Organization Setup:

Create or join a GitHub Organization.

Assign members to teams with specific repository permissions (Admin, Maintain, Write, Read).



Deliverable: Shared project plan with assigned roles, AWS credentials ready, and GitHub repo/organization configured.


1. Check you fintech infra deployment and make you have access to your Kubernetes cluster.:
a. Go to the ec2 instance: eks_client_node

i.Access server using session manager:
  sudo -i
  su - ubuntu
  aws configure to set up programmatic access using access key and secret access key.

  ii. Update kubeconfig file for authentication with apiserver:

     aws eks --region us-east-2 update-kubeconfig --name prod-dominion-cluster
     Added new context arn:aws:eks:us-east-2:418272782718:cluster/prod-dominion-cluster to /home/ubuntu/.kube/config
     cat /home/ubuntu/.kube/config


     2. Setup self-hosted runner for cicd app deployment.:

     a. Go to your project repository:
       setting ---> Action --> runner -->add self hosted runner --> linux

       Script to runner in runner terminal

       # Create a folder
mkdir actions-runner && cd actions-runner# Download the latest runner package
curl -o actions-runner-linux-x64-2.328.0.tar.gz -L https://github.com/actions/runner/releases/download/v2.328.0/actions-runner-linux-x64-2.328.0.tar.gz# Optional: Validate the hash
echo "01066fad3a2893e63e6ca880ae3a1fad5bf9329d60e77ee15f2b97c148c3cd4e  actions-runner-linux-x64-2.328.0.tar.gz" | shasum -a 256 -c# Extract the installer
tar xzf ./actions-runner-linux-x64-2.328.0.tar.gz

cd action-runner
cp actions-runner-linux-x64-2.328.0.tar.gz actions-runner
tar xzf ./actions-runner-linux-x64-2.328.0.tar.gz
./config.sh --url https://github.com/ndiforfusi/fintech-app --token AMVFMZY7RCT7E4B4BHFGNFDI5ZBS2



ðŸƒ GitHub Runner Auto-Start Setup (Ubuntu)
This guide explains how to configure the run.sh script to run continuously and automatically after reboot using systemd.

ðŸ“ Prerequisites
Ubuntu system with systemd (default in most distributions).

GitHub Actions self-hosted runner already configured.

Script located at: /home/ubuntu/actions-runner/run.sh

âš™ï¸ 1. Create the systemd Service
Create a new systemd unit file:

sudo vi /etc/systemd/system/github-runner.service
Paste the following content:

[Unit]
Description=GitHub Actions Self-Hosted Runner
After=network.target

[Service]
ExecStart=/home/ubuntu/actions-runner/run.sh
WorkingDirectory=/home/ubuntu/actions-runner
User=ubuntu
Group=docker
Restart=always
RestartSec=10
Environment=RUNNER_MANUALLY_TRAP_SIG=1

[Install]
WantedBy=multi-user.target



âœ… Make sure the paths (ExecStart, WorkingDirectory) are accurate for your environment.

ðŸ” 2. Make the Script Executable

chmod +x /home/ubuntu/actions-runner/run.sh

ðŸ”„ 3. Enable and Start the Service
Run the following commands to enable the service to start on boot and start it now:

sudo systemctl daemon-reexec
sudo systemctl daemon-reload
sudo systemctl enable github-runner.service
sudo systemctl start github-runner.service

Check to ensure the service is running:
  sudo systemctl status github-runner.service


  3. Setup SonarQube:

    a. Validate access to sonarqube server using iit public ip:
    http://18.227.72.119:9000

    b. Create project token:
    create project using global setting: fintech-app
    locally:
      maven:
        mvn clean verify sonar:sonar \
  -Dsonar.projectKey=fintech-app \
  -Dsonar.projectName='fintech-app' \
  -Dsonar.host.url=http://18.227.72.119:9000 \
  -Dsonar.token=sqp_ae21dd072037dee8ed74a7cef03f6f113a0a44f2


  c. Setup environmental variables for github-action cicd pipeline:

    settings --> secret and variable --> action --> add repository secret

AWS_ACCESS_KEY_ID --> your access key
AWS_SECRET_ACCESS_KEY --> your secret access key
Shemphad Global Concept, [27/10/2025 15:04]
mvn clean verify sonar:sonar \
 mvn clean verify sonar:sonar \
  -Dsonar.projectKey=fintech-app \
  -Dsonar.projectName='fintech-app' \
  -Dsonar.host.url=http://18.224.34.175:9000 \
  -Dsonar.token=sqp_2e812351b71ea1eabbb1095a6ddff65a4307eb2a
SLACK_WEBHOOK_URL --> your slack webhook
SONAR_HOST_URL --> http://18.227.72.119:9000
SONAR_TOKEN --> sqp_ae21dd072037dee8ed74a7cef03f6f113a0a44f2


4. Update pipeline with correct project specification:

  1. Go to your aws console: Select the arn of your newly deployed certificate.

  arn:aws:acm:us-east-2:327019199684:certificate/e566f7e1-0c1f-4028-b17a-162d7234115


  2. Go to your project repository: https://github.com/ndiforfusi/fintech-app

  i. k8s/base/ingress.yaml

  replace certificate arn:
    alb.ingress.kubernetes.io/certificate-arn: arn:aws:acm:us-east-2:327019199684:certificate/e566f7e1-0c1f-4028-b17a-162d72341159
    - host: app.fusisoft.com

    ii. eks_addons/elk/elasticsearch.yaml

    alb.ingress.kubernetes.io/certificate-arn: arn:aws:acm:us-east-2:327019199684:certificate/e566f7e1-0c1f-4028-b17a-162d72341159


    iii. eks_addons/monitoring/grafana_ingress.yaml
    alb.ingress.kubernetes.io/certificate-arn: arn:aws:acm:us-east-2:327019199684:certificate/e566f7e1-0c1f-4028-b17a-162d72341159

     - host: grafana.fusisoft.com --> change to your domain name.



     5. Validate deployment:
a. Check to ensure pods are in running state.
ubuntu@ip-10-0-101-156:~$ kubectl get po -n fintech
NAME                          READY   STATUS    RESTARTS       AGE
prod-app-9b77cd647-4gxtw      1/1     Running   2 (110s ago)   2m47s
prod-app-9b77cd647-ckgcs      1/1     Running   2 (2m4s ago)   2m47s
prod-app-9b77cd647-g8vfp      1/1     Running   2 (111s ago)   2m47s
prod-mysql-84d4b4d6cf-wnzz5   1/1     Running   0              2m47s
ubuntu@ip-10-0-101-156:~$


b. Create alias records in route 53 to access your applications.

create alias record in your public hosted zone and select the application load balancer endpont for your app.
e.g. -> app.fusisoft.com
--------------------------------------------------------------------
apiVersion: v1
kind: ServiceAccount
metadata:
  name: fluentd-es
  namespace: kube-system
  labels:
    k8s-app: fluentd-es
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
---
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: fluentd-es
  labels:
    k8s-app: fluentd-es
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
rules:
- apiGroups:
  - ""
  resources:
  - "namespaces"
  - "pods"
  verbs:
  - "get"
  - "watch"
  - "list"
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: fluentd-es
  labels:
    k8s-app: fluentd-es
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
subjects:
- kind: ServiceAccount
  name: fluentd-es
  namespace: kube-system
  apiGroup: ""
roleRef:
  kind: ClusterRole
  name: fluentd-es
  apiGroup: ""
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: fluentd-es-v2.2.0
  namespace: kube-system
  labels:
    k8s-app: fluentd-es
    version: v2.2.0
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
spec:
  selector:
    matchLabels:
      k8s-app: fluentd-es
      version: v2.2.0
  template:
    metadata:
      labels:
        k8s-app: fluentd-es
        kubernetes.io/cluster-service: "true"
        version: v2.2.0
      annotations:
        scheduler.alpha.kubernetes.io/critical-pod: ''
        seccomp.security.alpha.kubernetes.io/pod: 'docker/default'
    spec:
      priorityClassName: system-node-critical
      serviceAccountName: fluentd-es
      containers:
      - name: fluentd-es
        image: k8s.gcr.io/fluentd-elasticsearch:v2.2.0
        env:
        - name: FLUENTD_ARGS
          value: --no-supervisor -q
        resources:
          limits:
            memory: 500Mi
          requests:
            cpu: 100m
            memory: 200Mi
        volumeMounts:
        - name: varlog
          mountPath: /var/log
        - name: varlibdockercontainers
          mountPath: /var/lib/docker/containers
          readOnly: true
        - name: config-volume
          mountPath: /etc/fluent/config.d
      terminationGracePeriodSeconds: 30
      volumes:
      - name: varlog
        hostPath:
          path: /var/log
      - name: varlibdockercontainers
        hostPath:
          path: /var/lib/docker/containers
      - name: config-volume
        configMap:
          name: fluentd-es-config-v0.1.4
---

#
# Elastic Search configuration
#
apiVersion: v1
kind: Service
metadata:
  name: elasticsearch-logging
  namespace: kube-system
  labels:
    k8s-app: elasticsearch-logging
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
    kubernetes.io/name: "Elasticsearch"
spec:
  ports:
  - port: 9200
    protocol: TCP
    targetPort: db
  selector:
    k8s-app: elasticsearch-logging
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: elasticsearch-logging
  namespace: kube-system
  labels:
    k8s-app: elasticsearch-logging
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
---
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: elasticsearch-logging
  labels:
    k8s-app: elasticsearch-logging
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
rules:
- apiGroups:
  - ""
  resources:
  - "services"
  - "namespaces"
  - "endpoints"
  verbs:
  - "get"
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  namespace: kube-system
  name: elasticsearch-logging
  labels:
    k8s-app: elasticsearch-logging
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
subjects:
- kind: ServiceAccount
  name: elasticsearch-logging
  namespace: kube-system
  apiGroup: ""
roleRef:
  kind: ClusterRole
  name: elasticsearch-logging
  apiGroup: ""
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: elasticsearch-logging
  namespace: kube-system
  labels:
    k8s-app: elasticsearch-logging
    version: v6.2.5
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
spec:
  serviceName: elasticsearch-logging
  replicas: 2
  selector:
    matchLabels:
      k8s-app: elasticsearch-logging
      version: v6.2.5
  template:
    metadata:
      labels:
        k8s-app: elasticsearch-logging
        version: v6.2.5
        kubernetes.io/cluster-service: "true"
    spec:
      serviceAccountName: elasticsearch-logging
      containers:
      - image: k8s.gcr.io/elasticsearch:v6.2.5
        name: elasticsearch-logging
        resources:
          limits:
            cpu: 1000m
          requests:
            cpu: 100m
        ports:
        - containerPort: 9200
          name: db
          protocol: TCP
        - containerPort: 9300
          name: transport
          protocol: TCP
        volumeMounts:
        - name: elasticsearch-logging
          mountPath: /data
        env:
        - name: "NAMESPACE"
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
      initContainers:
      - image: alpine:3.6
        command: [ "/sbin/sysctl", "-w", "vm.max_map_count=262144" ]
        name: elasticsearch-logging-init
        securityContext:
          privileged: true
  volumeClaimTemplates:
  - metadata:
      name: elasticsearch-logging
    spec:
      storageClassName: cloud-ssd
      accessModes: [ "ReadWriteOnce" ]
      resources:
        requests:
          storage: 31Gi
---

#
# Kibana Deployment
#
apiVersion: apps/v1
kind: Deployment
metadata:
  name: kibana-logging
  namespace: kube-system
  labels:
    k8s-app: kibana-logging
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
spec:
  replicas: 1
  selector:
    matchLabels:
      k8s-app: kibana-logging
  template:
    metadata:
      labels:
        k8s-app: kibana-logging
      annotations:
        seccomp.security.alpha.kubernetes.io/pod: 'docker/default'
    spec:
      containers:
      - name: kibana-logging
        image: docker.elastic.co/kibana/kibana-oss:6.2.4
        resources:
          limits:
            cpu: 1000m
          requests:
            cpu: 100m
        env:
        - name: ELASTICSEARCH_URL
          value: http://elasticsearch-logging:9200
        ports:
        - containerPort: 5601
          name: ui
          protocol: TCP
---
apiVersion: v1
kind: Service
metadata:
  name: kibana-logging
  namespace: kube-system
  labels:
    k8s-app: kibana-logging
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
    kubernetes.io/name: "Kibana"
spec:
  ports:
  - port: 5601
    protocol: TCP
    targetPort: ui
  selector:
    k8s-app: kibana-logging
  type: ClusterIP
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: kibana-ingress
  namespace: kube-system
  annotations:
    alb.ingress.kubernetes.io/scheme: internet-facing
    alb.ingress.kubernetes.io/certificate-arn: arn:aws:acm:us-east-2:418272782718:certificate/52524784-6039-4c98-a999-51c3908931ab
    alb.ingress.kubernetes.io/listen-ports: '[{"HTTP":80,"HTTPS":443}]'
    alb.ingress.kubernetes.io/actions.ssl-redirect: >
      {"Type": "redirect", "RedirectConfig": {"Protocol": "HTTPS", "Port": "443", "StatusCode": "HTTP_301"}}
    alb.ingress.kubernetes.io/target-type: ip
    alb.ingress.kubernetes.io/healthcheck-port: "5601"
    alb.ingress.kubernetes.io/healthcheck-path: /
    alb.ingress.kubernetes.io/success-codes: "200,302"
    alb.ingress.kubernetes.io/load-balancer-name: kibana-alb
    alb.ingress.kubernetes.io/target-group-attributes: deregistration_delay.timeout_seconds=30
spec:
  ingressClassName: alb
  rules:
  - host: kibana.shemphadglobalconcept.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: kibana-logging
            port:
              number: 5601
-----------------------------------------------------------------------------
kubectl --namespace monitoring get secrets prometheus-stack-grafana -o jsonpath="{.data.admin-password}" | base64 -d ; echo
grafana command for passwd
=======================================================================================================================

terraform init
terraform get -update
terraform import module.eks-client-node.aws_iam_role.eks_client_ssm_role eks-client-ssm-role
